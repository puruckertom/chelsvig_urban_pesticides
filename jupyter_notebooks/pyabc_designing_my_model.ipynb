{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_path   C:\\Users\\jstelman\\Git\\chelsvig_urban_pesticides\n",
      "dir_path    C:\\Users\\jstelman\\Git\\chelsvig_urban_pesticides\\probabilistic_python\n",
      "exe_path    C:\\Users\\jstelman\\Git\\chelsvig_urban_pesticides\\probabilistic_python\\exe\n",
      "swmm_path   C:\\Users\\jstelman\\Git\\chelsvig_urban_pesticides\\probabilistic_python\\input\\swmm\n",
      "inp_path    C:\\Users\\jstelman\\Git\\chelsvig_urban_pesticides\\probabilistic_python\\input\\swmm\\NPlesantCreek.inp\n",
      "bin_path    C:\\Users\\jstelman\\Git\\chelsvig_urban_pesticides\\probabilistic_python\\input\\swmm\\NPlesantCreek.out\n",
      "vvwm_path   C:\\Users\\jstelman\\Git\\chelsvig_urban_pesticides\\probabilistic_python\\input\\vvwm\n",
      "wet_path    C:\\Users\\jstelman\\Git\\chelsvig_urban_pesticides\\probabilistic_python\\weather\n"
     ]
    }
   ],
   "source": [
    "# gen imports\n",
    "import pandas as pd, numpy as np, os, pyabc\n",
    "os.chdir(\"C:/Users/jstelman/Git/chelsvig_urban_pesticides/src/\")   # for small laptop\n",
    "# os.chdir(\"C:/Git/chelsvig_urban_pesticides/src/\")   # For Desktop\n",
    "from path_names import *\n",
    "from prpy_bookkeeping import *\n",
    "nsims = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a model\n",
    "\n",
    "(From \"quickstart\" example from pyabc docs)\n",
    "To do model selection, we first need some models. A model, in the simplest case,\n",
    "is just a callable which takes a single `dict` as input and returns a single `dict` as output. The keys of the input dictionary are the parameters of the model, the output\n",
    "keys denote the summary statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to take in parameters (in dictionary form) and output summary statistics\n",
    " Input: \n",
    "  SWMM parameters:\n",
    "    (see explanations here: <https://docs.google.com/document/d/1pVnjSP3cCKBCWUV3Brns8hJpkgHLekXns_9WG-qla7E/>)\n",
    "    NImperv\n",
    "    NPerv\n",
    "    SImperv\n",
    "    SPerv\n",
    "    PctZero\n",
    "    MaxRate\n",
    "    MinRate\n",
    "    Decay\n",
    "    DryTime\n",
    "    Por\n",
    "    WP\n",
    "    FC\n",
    "    Ksat\n",
    "    Roughness\n",
    "    Kdecay\n",
    "    BCoeff2\n",
    "    WCoeff2\n",
    "  \n",
    "  VVWM parameters:\n",
    "    (see explanations here: <https://docs.google.com/document/d/1JM8guprnugzJDvCaBwD-HCpgR5NvTGnCe3qjFXhMQHU/>)\n",
    "    kd - Sorption coefficient (mL/g) -\n",
    "    aer_aq - Water column degradation half-life (days) -\n",
    "    aer_aq_temp - Reference temp. For water column degradation (C) -\n",
    "    anae_aq - Benthic degradation half-life (days) -\n",
    "    anae_aq_temp - Reference temp. For benthic degradation (C) -\n",
    "    photo - Photolysis half-life (days) -\n",
    "    rflat - Reference latitude for photolysis -\n",
    "    hydro - Hydrolysis half-life (days) -\n",
    "    mwt - Molecular weight  -\n",
    "    vapor - Vapor pressure (torr) -\n",
    "    sol - Solubility (mg/L) -\n",
    "    henry - Henryâ€™s law constant -\n",
    "    benthic_depth - Depth of benthic region (m) -\n",
    "    porosity - Porosity of benthic region (--) -\n",
    "    bulk_density - Bulk density of benthic region -\n",
    "    froc2 - Fraction of organic carbon on sediment in benthic region -\n",
    "    doc2 - Concentration of dissolved organic carbon in benthic region (mg/L) -\n",
    "    bnmas - Areal concentration of biosolids in benthic region (g/m2) -\n",
    "    sused - Suspended solids concentration in water column (mg/L) -\n",
    "    chl - Chlorophyll concentration in water column (mg/L) -\n",
    "    froc1 - Fraction of organic carbon on suspended sediment in water column -\n",
    "    doc1 - Concentration of dissolved organic carbon in water column (mg/L) -\n",
    "    area - Area of water body (m2) -\n",
    "\n",
    " Output:\n",
    "  The values of 106 date- and site- specific bifenthrin concentrations (keys are date/site code indicators)\n",
    "  Why? Because that's the observation data we have access to\n",
    "\"\"\"\n",
    "def model(parameters: dict) -> dict:\n",
    "    \"\"\"\n",
    "    turn swmm parameters into one csv file. \n",
    "    turn vvwm parameters into another csv file.\n",
    "    if any of the the parameters are not okay:\n",
    "        (that means max and min and FC and WP relationships follow rules and that everything else is in the bounds we need)\n",
    "        return a dictionary full of nans and don't do the next part.\n",
    "    else:\n",
    "        run python scripts 03-06 and 08-09 (i/Ite = c)\n",
    "        run some derivation of postprocess notebook to take 106 cells of that file made by 09 and turn it into a dictionary\n",
    "    return (the dictionary)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File 02 would be here, but actually, it will take place in the prior segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simulate dict of LHS parameters from the first row of the dfs of swmm parameters and of vvwm parameters\n",
    "\"\"\"\n",
    "lhs_design = pd.concat([pd.read_csv(os.path.join(dir_path, \"io\", \"lhs_sampled_params.csv\"), index_col=0),\n",
    "                        pd.read_csv(os.path.join(dir_path, \"io\", \"lhs_sampled_params_vvwm.csv\"), index_col=0)],\n",
    "                      axis = 1)\n",
    "# round lhs decimals\n",
    "lhs_design = lhs_design.round(\n",
    "    {   # swmm\n",
    "        \"NImperv\": 4, \"NPerv\": 4, \"SImperv\":3, \"SPerv\": 3, \"PctZero\": 3, \"MaxRate\": 3, \"MinRate\": 3, \"Decay\": 2, \n",
    "        \"DryTime\": 0, \"Por\": 3, \"WP\": 3, \"FC\": 3, \"Ksat\": 3, \"Rough\": 4, \"Kdecay\": 3, \"BCoeff2\": 3, \"WCoeff2\": 3,\n",
    "        # vvwm\n",
    "        \"kd\": 2, \"aer_aq\": 0, \"aer_aq_temp\": 2, \"anae_aq\": 0, \"anae_aq_temp\": 2, \"photo\": 2, \"rflat\": 0, \"hydro\": 2, \n",
    "        \"sol\": 4, \"benthic_depth\": 3, \"porosity\": 2, \"bulk_density\": 2, \"froc2\": 3, \"doc2\": 2, \"bnmas\": 3, \"sused\": 3, \n",
    "        \"chl\": 3, \"froc1\": 3, \"doc1\": 2\n",
    "    })\n",
    "# SWMM lhs parameter (1 row) as dictionary\n",
    "params1 = lhs_design.to_dict(orient = 'records')[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NImperv': 0.0179,\n",
       " 'NPerv': 0.3951,\n",
       " 'SImperv': 1.363,\n",
       " 'SPerv': 4.442,\n",
       " 'PctZero': 15.2,\n",
       " 'MaxRate': 81.273,\n",
       " 'MinRate': 7.266,\n",
       " 'Decay': 6.26,\n",
       " 'DryTime': 8.0,\n",
       " 'Por': 0.465,\n",
       " 'FC': 0.361,\n",
       " 'WP': 0.119,\n",
       " 'Ksat': 1.719,\n",
       " 'Rough': 0.0194,\n",
       " 'Kdecay': 0.045,\n",
       " 'BCoeff2': 1.584,\n",
       " 'WCoeff2': 0.203,\n",
       " 'kd': 5333.21,\n",
       " 'aer_aq': 272.0,\n",
       " 'aer_aq_temp': 22.96,\n",
       " 'anae_aq': 255.0,\n",
       " 'anae_aq_temp': 23.76,\n",
       " 'photo': 404.98,\n",
       " 'rflat': 40.0,\n",
       " 'hydro': 106.3,\n",
       " 'sol': 0.48,\n",
       " 'benthic_depth': 0.884,\n",
       " 'porosity': 0.71,\n",
       " 'bulk_density': 1.61,\n",
       " 'froc2': 0.02,\n",
       " 'doc2': 27.62,\n",
       " 'bnmas': 1.094,\n",
       " 'sused': 51.269,\n",
       " 'chl': 0.984,\n",
       " 'froc1': 0.113,\n",
       " 'doc1': 0.64}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin function guts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Then we would have to split them into two after they have been passed into the function\n",
    "'''\n",
    "swmm_keys = list(params1.keys())[:17]\n",
    "vvwm_keys = list(params1.keys())[17:]\n",
    "swmm_params = {key: params1[key] for key in swmm_keys}\n",
    "vvwm_params = {key: params1[key] for key in vvwm_keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest_shutil, shutil, regex as re, dask, uuid\n",
    "loginfo = log_prefixer(\"03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "'''\n",
    "Helper function of edit_file:\n",
    "Create new line of .inp file with lhs simulated versions of values and text in old .inp file\n",
    " Inputs: fileline <str> -line from original .inp file-\n",
    "   Col <int> -column in the .inp file that contains the info you want to preserve and clean- \n",
    "   sim <float> -the simulated value to repace the old (observed) one with-\n",
    " Output: newline <str> -custom version of line for new file-\n",
    "'''\n",
    "def edit1line(fileline, Col, sim):\n",
    "    listline = fileline.split()\n",
    "    listline[Col] = str(sim)\n",
    "    newline = ' '.join([str(item) for item in listline]) + \"\\n\"\n",
    "    return(newline)\n",
    "\n",
    "# From 03 with edits\n",
    "\n",
    "'''\n",
    "Edit the new file to be cleaner\n",
    " Inputs: Ite <int> -index of current simulation-\n",
    "   Num <int> -Number of rows to clean-\n",
    "   row_0 <int> -Number of rows to skip-\n",
    "   parameter <str> -name of lhs design parameter (will become name of column in the .csv  file)-\n",
    "   Col <int> -column in the .inp file that contains the info you want to preserve and clean- \n",
    "   flines <list of str> -lines of the file to clean up-\n",
    " Output: cleaned up lines of file given\n",
    "'''\n",
    "def editted_lines(swmm_dict, Num, row_0, parameter, Col, flines):\n",
    "    # value of \"parameter\" in current lhs simulation\n",
    "    # sim = lhs_design.loc[Ite - 1, parameter]   # updating\n",
    "    sim = swmm_dict[parameter]\n",
    "    print(sim)\n",
    "    return([edit1line(flines[row_0 + i], Col, sim) for i in range(Num)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57.8,\n",
       " 40.6289,\n",
       " 227.649,\n",
       " 10.2923,\n",
       " 158.7419,\n",
       " 30.2337,\n",
       " 53.5458,\n",
       " 43.0476,\n",
       " 56.2647,\n",
       " 72.3722,\n",
       " 39.4796,\n",
       " 31.7947,\n",
       " 87.905,\n",
       " 280.1313,\n",
       " 40.526,\n",
       " 39.5054,\n",
       " 5.5064,\n",
       " 60.9305,\n",
       " 25.9281,\n",
       " 92.8453,\n",
       " 104.4928,\n",
       " 53.7259,\n",
       " 23.6466,\n",
       " 44.3771,\n",
       " 23.1835,\n",
       " 56.0074,\n",
       " 46.8043,\n",
       " 135.3097,\n",
       " 154.5049,\n",
       " 35.8344,\n",
       " 26.8544,\n",
       " 48.1595,\n",
       " 57.5169,\n",
       " 51.7446,\n",
       " 50.3809,\n",
       " 6.6471,\n",
       " 66.2397,\n",
       " 74.9281,\n",
       " 8.4997,\n",
       " 36.0574,\n",
       " 21.0993,\n",
       " 79.0193,\n",
       " 69.2587,\n",
       " 80.0571,\n",
       " 2.6503,\n",
       " 24.0068,\n",
       " 4.9489,\n",
       " 74.4821,\n",
       " 90.2294,\n",
       " 113.4728,\n",
       " 41.9241,\n",
       " 94.312,\n",
       " 25.748,\n",
       " 40.3802,\n",
       " 108.524,\n",
       " 79.8684,\n",
       " 147.995,\n",
       " 75.803,\n",
       " 6.8701,\n",
       " 127.1102,\n",
       " 19.5039,\n",
       " 4.4085,\n",
       " 60.8105,\n",
       " 68.6669,\n",
       " 0.8234,\n",
       " 127.5304,\n",
       " 111.2343,\n",
       " 147.8492,\n",
       " 25.7394,\n",
       " 7.6849,\n",
       " 53.563,\n",
       " 125.2147,\n",
       " 43.7166,\n",
       " 49.0343,\n",
       " 24.0411,\n",
       " 33.2442,\n",
       " 514.3245,\n",
       " 134.0832,\n",
       " 58.8378,\n",
       " 88.2481,\n",
       " 2.9848,\n",
       " 43.0648,\n",
       " 140.7132,\n",
       " 4.5801,\n",
       " 106.5427,\n",
       " 42.6016,\n",
       " 264.6327,\n",
       " 60.7247,\n",
       " 75.6829,\n",
       " 24.2899,\n",
       " 163.5621,\n",
       " 26.2883,\n",
       " 15.8159,\n",
       " 17.7199,\n",
       " 40.0629,\n",
       " 15.1126,\n",
       " 34.2992,\n",
       " 61.3251,\n",
       " 40.6289,\n",
       " 41.8812,\n",
       " 29.6333,\n",
       " 111.6974,\n",
       " 95.4441,\n",
       " 268.8526,\n",
       " 65.3477,\n",
       " 86.8243,\n",
       " 30.2508,\n",
       " 71.8233,\n",
       " 79.1308,\n",
       " 71.5316,\n",
       " 4.1427,\n",
       " 39.3939,\n",
       " 79.0107]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "To eliminate the need to open .inp file in 05, let's create a variable with the info we need.\n",
    "Afterall, it never changes\n",
    "'''\n",
    "\n",
    "# create blank list to hold subcatchment areas\n",
    "sub_list_area = []\n",
    "\n",
    "with open(inp_path, \"r\") as ip_file:\n",
    "    lines1 = ip_file.readlines()[55:]\n",
    "    # [sub_list_area.append(float(lines1[thissub].split()[3])) for thissub in range(113)]   ## Too smooshed\n",
    "    for thissub in range(113):\n",
    "        # grab the area\n",
    "        listline = lines1[thissub].split()\n",
    "        area = float(listline[3])\n",
    "        # insert into blank list\n",
    "        sub_list_area.append(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# '''\n",
    "let's make a file to hold the copied input files\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1:\n",
    "Generate random id (see \"uuid.uuid4().hex[0:8]     #generate random jobID\" in Jeff's code on bookmarks bar)\n",
    "Make input and output file names incorporate this id (so things don't get lost or overwritten thanks to parallelism. Yay)\n",
    "Also make DLL copy incorporate this id \n",
    "\n",
    "Step 2:\n",
    "Run the thing!\n",
    "Run swmm toolbox on it\n",
    "Then delete that massive output file and the dll.\n",
    "Honestly, maybe even the input file. \n",
    "\n",
    "(Step 3:\n",
    "Carry on with the rest of the munging til the VVWM step. \n",
    "\n",
    "Step 4:\n",
    "Do something similar to step 1 for VVWM)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simulate the first step of running the model for one iteration\n",
    "\"\"\"\n",
    "\n",
    "# generate random id\n",
    "Ite = i = rpt = uuid.uuid4().hex[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder  09cd7b5f  created \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make the folder, WE SHOULD TRY TO SKIP THIS\n",
    "# MAKE JUST ONE DIR FOR ALL CREATED inp FILES, ONE FOR ALL CREATED out FILES, AND ONE FOR ALL CREATED dll FILES\n",
    "new_dir = os.path.join(swmm_path, \"input_\" + Ite)\n",
    "\n",
    "if not os.path.exists(new_dir):\n",
    "    os.mkdir(new_dir)\n",
    "    print(\"Folder \", Ite, \" created\", \"\\n\")\n",
    "else:\n",
    "    print(\"Folder \", Ite, \"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy base file into new file location\n",
    "#### THIS SHIT HAS ALREADY BEEN NAMED \"inp_path\" IN PATH_NAMES\n",
    "# old_path = os.path.join(swmm_path, \"NPlesantCreek.inp\")\n",
    "# old_path = inp_path\n",
    "# RIGHT HERE!!!!!!!! temp it instead.... but what to do with the output?\n",
    "new_path = os.path.join(new_dir, \"NPlesantCreek.inp\")\n",
    "# loginfo(\"Copying base swmm input file <\" + old_path + \"> into <\" + new_dir + \">.\")\n",
    "# shutil.copyfile(old_path, new_path)\n",
    "# shutil.copyfile(inp_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0179\n",
      "0.3951\n",
      "1.363\n",
      "4.442\n",
      "15.2\n",
      "81.273\n",
      "7.266\n",
      "6.26\n",
      "8.0\n",
      "0.465\n",
      "0.119\n",
      "0.361\n",
      "1.719\n",
      "0.045\n",
      "1.584\n",
      "0.203\n"
     ]
    }
   ],
   "source": [
    "with open(inp_path, \"r\") as read_file, open(new_path, \"w\") as write_file:\n",
    "    filelines = read_file.readlines()\n",
    "\n",
    "    # first we need to correct some absolute paths, because they are currently only set to work on the author's computer\n",
    "    filelines = replace_infile_abspaths(filelines = filelines)\n",
    "    \n",
    "    # 113 = number of subcatchments\n",
    "    for c, par in enumerate([\"NImperv\", \"NPerv\", \"SImperv\", \"SPerv\", \"PctZero\"]):\n",
    "        filelines[172:(172 + 113)] = editted_lines(swmm_dict = swmm_params, Num = 113, row_0 = 172, parameter = par, Col = c+1, flines = filelines)\n",
    "    for c, par in enumerate([\"MaxRate\", \"MinRate\", \"Decay\", \"DryTime\"]):\n",
    "        filelines[289:(289 + 113)] = editted_lines(swmm_dict = swmm_params, Num = 113, row_0 = 289, parameter = par, Col = c+1, flines = filelines)\n",
    "\n",
    "    # 1 = number of aquifers\n",
    "    for c, par in enumerate([\"Por\", \"WP\", \"FC\", \"Ksat\"]):\n",
    "        filelines[406:(406 + 1)] = editted_lines(swmm_dict = swmm_params, Num = 1, row_0 = 406, parameter = par, Col = c+1, flines = filelines)\n",
    "\n",
    "    # 195 = number of conduits\n",
    "    # filelines[734:(734 + 195)] = editted_lines(swmm_dict = swmm_params, Num = 195, row_0 = 734, parameter = \"Rough\", Col = 4, flines = filelines)\n",
    "\n",
    "    # 1 = number of pollutants\n",
    "    filelines[1125:(1125 + 1)] = editted_lines(swmm_dict = swmm_params, Num = 1, row_0 = 1125, parameter = \"Kdecay\", Col = 5, flines = filelines)\n",
    "    filelines[1371:(1371 + 1)] = editted_lines(swmm_dict = swmm_params, Num = 1, row_0 = 1371, parameter = \"BCoeff2\", Col = 4, flines = filelines)\n",
    "    filelines[1377:(1377 + 1)] = editted_lines(swmm_dict = swmm_params, Num = 1, row_0 = 1377, parameter = \"WCoeff2\", Col = 4, flines = filelines)\n",
    "    \n",
    "    write_file.writelines(filelines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswmm import Simulation\n",
    "import swmmtoolbox.swmmtoolbox as swmmtoolbox\n",
    "import time\n",
    "from pyswmm.lib import DLL_SELECTION\n",
    "loginfo = log_prefixer(\"04\")\n",
    "# WE ALREADY HAVE new_dir\n",
    "# inp_dir_prefix = os.path.join(dir_path, \"input\", \"swmm\", \"input_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DLL folder\n",
    "\n",
    "# save the path for the original dynamic link library\n",
    "dll_path = DLL_SELECTION()\n",
    "# save its base name (just the name of the file)\n",
    "dll_bn = os.path.basename(dll_path)\n",
    "# save the path to a new folder where copies of dll will be stored upon creation\n",
    "dll_dir = os.path.join(dir_path, \"input\", \"swmm\", \"dll\")\n",
    "# make sure that folder exists. If it doesn't, create it.\n",
    "if not os.path.exists(dll_dir):\n",
    "    loginfo(\"Creating directory <\" + dll_dir + \">.\")\n",
    "    print(\"Creating <dll/> directory.\")\n",
    "    os.mkdir(dll_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jstelman\\\\Git\\\\chelsvig_urban_pesticides\\\\probabilistic_python\\\\input\\\\swmm\\\\dll\\\\swmm5-x64-09cd7b5f.dll'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log something generic\n",
    "# loginfo(\"Simmulation \" + str(i) + \" of \" + str(nsims))\n",
    "\n",
    "# create path to the 1-time-use dll file we are going to create\n",
    "dll_i = dll_bn[:dll_bn.rindex(\".\")] + '-' + i + dll_bn[dll_bn.rindex(\".\"):]\n",
    "lib_path = os.path.join(dll_dir, dll_i)\n",
    "# create 1-time-use dll file copy\n",
    "shutil.copyfile(dll_path, lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:04/02/2021 08:45:26:: 04: Executing SWMM simmulation with no interaction. Input from <C:\\Users\\jstelman\\Git\\chelsvig_urban_pesticides\\probabilistic_python\\input\\swmm\\input_09cd7b5f\\NPlesantCreek.inp>. Will store output in <C:\\Users\\jstelman\\Git\\chelsvig_urban_pesticides\\probabilistic_python\\input\\swmm\\input_09cd7b5f\\NPlesantCreek.out>.\n",
      "INFO:root:04/02/2021 09:06:55:: 04: Deleting <C:\\Users\\jstelman\\Git\\chelsvig_urban_pesticides\\probabilistic_python\\input\\swmm\\input_09cd7b5f\\NPlesantCreek.out> to free up memory.\n",
      "INFO:root:04/02/2021 09:06:56:: 04: Deleting <swmm5-x64-09cd7b5f.dll> to free up memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted <input_09cd7b5f/JS_NPlesantCreek.out> and <swmm5-x64-09cd7b5f.dll> to free up memory.\n"
     ]
    }
   ],
   "source": [
    "# specify the directory with the file pyswmm needs by attaching the folder id to the rest of the folder's absolute path\n",
    "# sim_dir = inp_dir_prefix + str(i)\n",
    "# specify the actual file pyswmm needs\n",
    "# WE ALREADY HAVE THIS, IT'S CALLED \"new_path\"\n",
    "# sim_path = os.path.join(sim_dir, r'NPlesantCreek.inp')\n",
    "#print(\"Simulation input file found:\", sim_path)\n",
    "\n",
    "# specify the file that pyswmm will (over)write with output after running the probabilistic simulation\n",
    "# THIS IS NOT REQUIRED YET, BUT WHY NOT BE EXPLICIT\n",
    "sim_bin_path = os.path.join(new_dir, \"NPlesantCreek.out\") #sim_dir, \"NPlesantCreek.out\")\n",
    "# delete pre-existing .out, if present, in order to run swmm agreeably\n",
    "if os.path.exists(sim_bin_path):\n",
    "    loginfo(\"Deleting current copy of <\" + sim_bin_path + \"> so new copy can be created.\")\n",
    "    #print(\"Deleting current copy of <NPlesantCreek.out> so new copy can be created.\")\n",
    "    os.remove(sim_bin_path)\n",
    "\n",
    "# stagger starting times 1 sec apart\n",
    "# time.sleep(i)\n",
    "\n",
    "# load the model {no interaction, write (binary) results to <JS_NPlesantCreek.out>, use the specified dll}\n",
    "sim = Simulation(inputfile=new_path, reportfile=None, outputfile=sim_bin_path, swmm_lib_path=lib_path)\n",
    "# simulate the loaded model\n",
    "loginfo(\"Executing SWMM simmulation with no interaction. Input from <\" + new_path + \">. Will store output in <\" + sim_bin_path + \">.\")\n",
    "# sim.execute()\n",
    "with sim as s:\n",
    "    for step in s:\n",
    "        pass\n",
    "\n",
    "# extract swmm outputs with swmmtoolbox and delete expensive binary files\n",
    "lab1 = 'subcatchment,,Runoff_rate'\n",
    "lab2 = 'subcatchment,,Bifenthrin'\n",
    "runf_stack = swmmtoolbox.extract(sim_bin_path, lab1)\n",
    "bif_stack = swmmtoolbox.extract(sim_bin_path, lab2)\n",
    "\n",
    "loginfo(\"Deleting <\" + sim_bin_path + \"> to free up memory.\")\n",
    "os.remove(sim_bin_path)\n",
    "loginfo(\"Deleting <\" + dll_i + \"> to free up memory.\")\n",
    "os.system(\"rm \" + lib_path)\n",
    "print(\"Deleted <input_\" + str(i) + \"/JS_NPlesantCreek.out> and <\" + dll_i + \"> to free up memory.\")\n",
    "\n",
    "loginfo(\"Deleting <\" + sim_bin_path[:-3]+\"rpt\" + \"> to free up memory.\")\n",
    "os.remove(sim_bin_path[:-3]+\"rpt\")\n",
    "loginfo(\"Deleting <\" + new_path + \"> to free up memory.\")\n",
    "os.remove(new_path)\n",
    "# print(\"Deleted <input_\" + str(i) + \"/NPlesantCreek.out> and <\" + dll_i + \"> to free up memory.\")\n",
    "\n",
    "# compute and export daily averages to csv files and finish\n",
    "runf_davg = runf_stack.resample('D').mean()\n",
    "bif_davg = bif_stack.resample('D').mean()\n",
    "# print(save_and_finish(runf_davg, os.path.join(sim_dir, \"swmm_output_davg_runf.csv\")))\n",
    "# print(save_and_finish(bif_davg, os.path.join(sim_dir, \"swmm_output_davg_bif.csv\")))\n",
    "# msg1 and msg2 text will be the same, but we must do both to save both csvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outfall_31_26': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan],\n",
       " 'outfall_31_28': [nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan],\n",
       " 'outfall_31_29': [nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan],\n",
       " 'outfall_31_35': [nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan],\n",
       " 'outfall_31_36': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'outfall_31_38': [nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'outfall_31_42': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  1,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  1,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Make this slicer object once, for all.\n",
    "Use slicer[o] to reference the thing you want to reference\n",
    "Honestly, in the future, we will probably change it to a dot product anyway. And replace the NaNs with 0s\n",
    "'''\n",
    "outfalls = ['outfall_31_26', 'outfall_31_28', 'outfall_31_29', 'outfall_31_35',\n",
    "            'outfall_31_36', 'outfall_31_38', 'outfall_31_42']\n",
    "slicer = {}\n",
    "for o in outfalls:\n",
    "    outfall_dir = os.path.join(vvwm_path, o)\n",
    "    # subset subcatchment outputs for each vvwm\n",
    "    outfall_path = os.path.join(outfall_dir, o + \".csv\")\n",
    "    # declare which columns need to be subset\n",
    "    sub_ids = (pd.read_csv(outfall_path, header = 0, usecols=['Subcatchment_ID']) - 1).sum(1).tolist()\n",
    "\n",
    "    # Now, to subset:\n",
    "    # make list where the elements at included indices are 1s and the other elements are 0s\n",
    "    slicer[o] = [(1 if x in sub_ids else np.nan) for x in range(113)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp_dir_prefix = os.path.join(dir_path, \"input\", \"swmm\", \"input_\")\n",
    "\n",
    "# # save the date columns for the csvs we will be making at the very end. \n",
    "# datedf = pd.DataFrame({\"date\": pd.date_range(start='1/1/2009', periods=3287, freq='D')})\n",
    "# datedf['year'] = datedf['date'].dt.year\n",
    "# datedf['month'] = datedf['date'].dt.month\n",
    "# datedf['day'] = datedf['date'].dt.day\n",
    "# dates = datedf.date.tolist()\n",
    "# years = datedf.year.tolist()\n",
    "# months = datedf.month.tolist()\n",
    "# days = datedf.day.tolist()\n",
    "\n",
    "# runf_df_cols, runf_df_rows, bif_df_cols, bif_df_rows = 113, 3287, 113, 3287\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DatetimeIndex(pd.date_range(start='1/1/2009', periods=3287, freq='D')) == runf_davg.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in outfalls:\n",
    "    # set pathways\n",
    "    outfall_dir = os.path.join(vvwm_path, o)\n",
    "\n",
    "    # create vvwm prob. sim. input folders\n",
    "    for Ite in range(1, nsims + 1):\n",
    "        new_dir = os.path.join(outfall_dir, \"input_\" + str(Ite))\n",
    "\n",
    "        if not os.path.exists(new_dir):\n",
    "            os.mkdir(new_dir)\n",
    "            print(\"Folder \", Ite, \" created\")\n",
    "        else:\n",
    "            print(\"Folder \", Ite, \"already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # write out swmm daily average outputs\n",
    "#     runf_to_conv = dask.delayed(pd.read_csv)(os.path.join(sim_dir, \"swmm_output_davg_runf.csv\"), index_col = 0)\n",
    "#     bif_to_conv = dask.delayed(pd.read_csv)(os.path.join(sim_dir, \"swmm_output_davg_bif.csv\"), index_col = 0)\n",
    "    runf_to_conv = runf_davg\n",
    "    bif_to_conv = bif_davg\n",
    "\n",
    "    # Conversion for runf and bif\n",
    "    runf_to_conv = runf_to_conv.mul(86400).mul(0.01).div(sub_list_area)\n",
    "    bif_to_conv = bif_to_conv.mul(runf_to_conv.values)\n",
    "\n",
    "    # Write out converted swmm outputs for runoff and bifenthrin\n",
    "#     runf_to_conv = dask.delayed(save_and_continue)(runf_to_conv, os.path.join(sim_dir, \"swmm_conv_to_vvwm_runf.csv\"))\n",
    "#     bif_to_conv = dask.delayed(save_and_continue)(bif_to_conv, os.path.join(sim_dir, \"swmm_conv_to_vvwm_bif.csv\"))\n",
    "\n",
    "    for o in outfalls:\n",
    "        outfall_dir = os.path.join(vvwm_path, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         # subset subcatchment outputs for each vvwm\n",
    "#         outfall_path = os.path.join(outfall_dir, o + \".csv\")\n",
    "#         # declare which columns need to be subset\n",
    "#         sub_ids = (pd.read_csv(outfall_path, header = 0, usecols=['Subcatchment_ID']) - 1).sum(1).tolist()\n",
    "        \n",
    "#         # Now, to subset:\n",
    "#         # make list where the elements at included indices are 1s and the other elements are 0s\n",
    "#         slicer = [(1 if x in sub_ids else np.nan) for x in range(113)]\n",
    "        # now multiply it by the df\n",
    "        runf_sub = (runf_to_conv * slicer[o]).dropna(1)\n",
    "        bif_sub = (bif_to_conv * slicer[o]).dropna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # add a total sum column and date columns\n",
    "        runf_sub=runf_sub.assign(runf_sum = runf_sub.sum(axis=1), date = dates, year = years, month = months, day = days)\n",
    "        bif_sub=bif_sub.assign(bif_sum = bif_sub.sum(axis=1), date = dates, year = years, month = months, day = days)\n",
    "\n",
    "#         # write out dataframes\n",
    "#         sfx_o = outfall_path[-9:]\n",
    "#         runf_out = os.path.join(outfall_dir, \"input_\" + str(rpt), \"runf_for_vvwm\" + sfx_o)\n",
    "#         bif_out = os.path.join(outfall_dir, \"input_\" + str(rpt), \"bif_for_vvwm\" + sfx_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now, we are essentially going to only keep the total and date columns. \n",
    "(So a lot of 05 might be editted to avoid wasted effort.)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need priors \n",
    "\n",
    "Not sure how to do that because there seems to be a much stricter method for prior declaration. \n",
    "Maybe I can do this first step outside\n",
    "then enter it as a a normal distribution with a sigma of 0 for all the parameters that are special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Priors. Get the values from that csv. Just for SWMM at first.\n",
    "\"\"\"\n",
    "swmm_ranges = pd.read_csv(os.path.join(dir_path, \"input\", \"lhs\", \"lhs_param_ranges.csv\"), index_col=0,\n",
    "                           usecols = [\"Parameter\",\"Min\", \"Range\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Distribution 'BCoeff2', 'Decay', 'DryTime', 'FC', 'Kdecay', 'Ksat', 'MaxRate', 'MinRate', 'NImperv', 'NPerv', 'PctZero', 'Por', 'Rough', 'SImperv', 'SPerv', 'WCoeff2', 'WP', 'aer_aq', 'aer_aq_temp', 'anae_aq', 'anae_aq_temp', 'benthic_depth', 'bnmas', 'bulk_density', 'chl', 'doc1', 'doc2', 'froc1', 'froc2', 'hydro', 'kd', 'photo', 'porosity', 'rflat', 'sol', 'sused'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Link up with the vvwm priors and make one big list with 36 params\n",
    "'''\n",
    "vvwm_ranges = pd.read_csv(os.path.join(dir_path, \"input\", \"lhs\", \"lhs_param_ranges_vvwm.csv\"), index_col=0,\n",
    "                           usecols = [\"Parameter\",\"Min\", \"Range\"])\n",
    "\n",
    "param_ranges = pd.concat([swmm_ranges, vvwm_ranges], axis = 0)\n",
    "\n",
    "priors = param_ranges.to_dict(\"index\")\n",
    "\n",
    "# borrowed from Jeff: <https://github.com/JeffreyMinucci/bee_neonic_abc/blob/master/pyabc_run.ipynb>\n",
    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", loc = v['Min'], scale = v['Range'])\n",
    "                        for key, v in priors.items()})\n",
    "prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Min</th>\n",
       "      <th>Range</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parameter</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NImperv</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPerv</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SImperv</th>\n",
       "      <td>1.270000</td>\n",
       "      <td>1.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPerv</th>\n",
       "      <td>2.540000</td>\n",
       "      <td>2.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PctZero</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>99.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaxRate</th>\n",
       "      <td>8.460000</td>\n",
       "      <td>118.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MinRate</th>\n",
       "      <td>0.254000</td>\n",
       "      <td>10.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decay</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DryTime</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Por</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FC</th>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WP</th>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ksat</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>12.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rough</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kdecay</th>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.197260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BCoeff2</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WCoeff2</th>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kd</th>\n",
       "      <td>882.000000</td>\n",
       "      <td>5028.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aer_aq</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>360.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aer_aq_temp</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anae_aq</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>725.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anae_aq_temp</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photo</th>\n",
       "      <td>96.900000</td>\n",
       "      <td>319.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rflat</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hydro</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>364.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sol</th>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.744063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benthic_depth</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>porosity</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bulk_density</th>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>froc2</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>59.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bnmas</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>4.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sused</th>\n",
       "      <td>0.005000</td>\n",
       "      <td>79.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chl</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>froc1</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>14.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Min        Range\n",
       "Parameter                             \n",
       "NImperv          0.010000     0.015000\n",
       "NPerv            0.050000     0.450000\n",
       "SImperv          1.270000     1.270000\n",
       "SPerv            2.540000     2.540000\n",
       "PctZero          0.010000    99.990000\n",
       "MaxRate          8.460000   118.540000\n",
       "MinRate          0.254000    10.666000\n",
       "Decay            2.000000     5.000000\n",
       "DryTime          2.000000    12.000000\n",
       "Por              0.400000     0.100000\n",
       "FC               0.060000     0.320000\n",
       "WP               0.024000     0.241000\n",
       "Ksat             0.250000    12.450000\n",
       "Rough            0.010000     0.016000\n",
       "Kdecay           0.002740     0.197260\n",
       "BCoeff2          0.500000     1.500000\n",
       "WCoeff2          0.066000     0.148000\n",
       "kd             882.000000  5028.000000\n",
       "aer_aq           5.000000   360.000000\n",
       "aer_aq_temp     20.000000     5.000000\n",
       "anae_aq          5.000000   725.000000\n",
       "anae_aq_temp    20.000000     5.000000\n",
       "photo           96.900000   319.100000\n",
       "rflat           40.000000     0.000000\n",
       "hydro            0.100000   364.900000\n",
       "sol              0.000241     0.744063\n",
       "benthic_depth    0.010000     0.990000\n",
       "porosity         0.100000     0.700000\n",
       "bulk_density     0.860000     0.900000\n",
       "froc2            0.001000     0.029000\n",
       "doc2             0.010000    59.990000\n",
       "bnmas            0.001000     4.999000\n",
       "sused            0.005000    79.995000\n",
       "chl              0.001000     1.499000\n",
       "froc1            0.001000     0.139000\n",
       "doc1             0.100000    14.900000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So then I could do something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = ABCSMC(model, prior, distance = hydroeval.NSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then I need to set the observed data \n",
    "\n",
    "I will have to make that csv into a dictionary like how Jeff did it. \n",
    "This should be the easiest part\n",
    "I've basically already done some of it in the post-processing notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
